{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conala-tokenized-300b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahafp/Conala-Challenge/blob/master/conala_tokenized_300b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieK0oXM5CwsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "import torch\n",
        "import numpy\n",
        "from torch import autograd, nn, optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import collections\n",
        "import math\n",
        "%matplotlib inline\n",
        "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "import time\n",
        "import math\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fZ8dG5kC4qX",
        "colab_type": "code",
        "outputId": "a2a49bf0-2db0-4e24-f20d-a36761dc25e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns_yiAEGC7GM",
        "colab_type": "code",
        "outputId": "21ac2d84-7b57-4ef5-e5d5-24aea5360ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eUrGetvHVUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "MAX_LENGTH = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STKQU7xMC9a3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VocabIntent :\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\",2: \"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQP-ykdwDF_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VocabCode:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\",2:\"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in re.split('([^a-zA-Z0-9 ])',sentence):\n",
        "          if word is not '':\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zme_042tDJDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def orginize_data(data_type):\n",
        "  json_data = '/content/drive/My Drive/conala/' + data_type  \n",
        "  path = open(json_data, \"r\")\n",
        "  data = json.load(path)\n",
        "  pairs=[]\n",
        "  for dic in data:\n",
        "      if dic[\"rewritten_intent\"] is None:\n",
        "          continue\n",
        "      pairs.append([dic[\"rewritten_intent\"], dic[\"snippet\"]])\n",
        "  return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW5f2MoQDLg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(vocab1_name, vocab2_name, data_type):\n",
        "  pairs=orginize_data(data_type=data_type)\n",
        "  intent_vocab=VocabIntent(vocab1_name)\n",
        "  code_vocab=VocabCode(vocab2_name)\n",
        "\n",
        "  for pair in pairs:\n",
        "    intent_vocab.addSentence(pair[0])\n",
        "    code_vocab.addSentence(pair[1])\n",
        "  \n",
        "  return intent_vocab, code_vocab, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR6w2QLIDPwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_lang, output_lang, pairs= prepare_data('intent', 'code', 'conala-train.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfu4LNaPlCIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load pre-trained embeddings\n",
        "def getPreTrainedWeights():\n",
        "  glove_file = datapath('/content/drive/My Drive/conala/glove.6B.300d.txt')\n",
        "  tmp_file = get_tmpfile('/content/drive/My Drive/conala/glove.6B.300.w2v.txt')\n",
        "  dim = glove2word2vec(glove_file, tmp_file)\n",
        "  model = KeyedVectors.load_word2vec_format(tmp_file)\n",
        "  return torch.FloatTensor(model.wv.vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X3bvLLYBuz6",
        "colab_type": "code",
        "outputId": "557270b2-25d4-4522-fee4-7b024fc597b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "weights = getPreTrainedWeights()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjYTuOnfF-F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding=nn.Embedding.from_pretrained(weights)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeS38kFuGJmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5DPvrokGQrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "  return [lang.word2index[word] if word in lang.word2index else UNK_token for word in sentence.split(' ') ]\n",
        "\n",
        "def c_indexesFromSentence(lang, sentence):\n",
        "    sen = re.split('([^a-zA-Z0-9 ])',sentence)\n",
        "    return [lang.word2index[word] for word in sen if word is not '']\n",
        "\n",
        "def tensorFromSentence(lang, sentence, key):\n",
        "    if key is 'intent':\n",
        "      indexes = indexesFromSentence(lang, sentence)\n",
        "      indexes.append(EOS_token)\n",
        "    else:\n",
        "      indexes = c_indexesFromSentence(lang, sentence)\n",
        "      indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0], 'intent')\n",
        "    target_tensor =tensorFromSentence(output_lang, pair[1], 'code')\n",
        "    return (input_tensor, target_tensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvQ9BydGVQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeLIxUwAGaYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRnWSrgZGd6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    return plot_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLwVGq_AGgmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvhLCGAIGkSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence, 'intent')\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niS62EYRGrMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ''.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC554XDTT-IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_data_set(encoder, decoder, pairs, n=10):\n",
        "  answer_list = []\n",
        "  for i,pair in enumerate(pairs, 0):\n",
        "      output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "      output_sentence = ''.join(output_words)\n",
        "      output_sentence = output_sentence.strip('<EOS>')\n",
        "      answer_list.append(output_sentence)\n",
        "      if i%50==0:\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "  return answer_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-uwZYvZGuag",
        "colab_type": "code",
        "outputId": "02d4723d-8d91-4b44-9044-929b3bef0b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden_size = 300\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "train_losses = trainIters(encoder1, attn_decoder1, 75000, print_every=100)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 10s (- 125m 12s) (100 0%) 5.4269\n",
            "0m 13s (- 86m 28s) (200 0%) 4.8073\n",
            "0m 17s (- 73m 22s) (300 0%) 4.4133\n",
            "0m 21s (- 66m 1s) (400 0%) 3.8255\n",
            "0m 24s (- 61m 39s) (500 0%) 4.0717\n",
            "0m 28s (- 58m 8s) (600 0%) 3.8309\n",
            "0m 31s (- 54m 55s) (700 0%) 3.7282\n",
            "0m 34s (- 52m 42s) (800 1%) 3.7835\n",
            "0m 37s (- 51m 35s) (900 1%) 3.8556\n",
            "0m 41s (- 50m 38s) (1000 1%) 3.9422\n",
            "0m 44s (- 49m 54s) (1100 1%) 3.8063\n",
            "0m 47s (- 49m 4s) (1200 1%) 3.6181\n",
            "0m 51s (- 48m 26s) (1300 1%) 3.7900\n",
            "0m 54s (- 48m 4s) (1400 1%) 3.8880\n",
            "0m 57s (- 47m 20s) (1500 2%) 3.5892\n",
            "1m 0s (- 46m 35s) (1600 2%) 3.4579\n",
            "1m 4s (- 46m 29s) (1700 2%) 3.6104\n",
            "1m 7s (- 46m 4s) (1800 2%) 3.6024\n",
            "1m 11s (- 45m 42s) (1900 2%) 3.6108\n",
            "1m 14s (- 45m 36s) (2000 2%) 3.6731\n",
            "1m 18s (- 45m 29s) (2100 2%) 3.7236\n",
            "1m 21s (- 45m 3s) (2200 2%) 3.7307\n",
            "1m 25s (- 44m 54s) (2300 3%) 3.7934\n",
            "1m 28s (- 44m 44s) (2400 3%) 3.5104\n",
            "1m 31s (- 44m 24s) (2500 3%) 3.7418\n",
            "1m 35s (- 44m 11s) (2600 3%) 3.6365\n",
            "1m 38s (- 43m 57s) (2700 3%) 3.5705\n",
            "1m 42s (- 43m 53s) (2800 3%) 3.7761\n",
            "1m 45s (- 43m 50s) (2900 3%) 3.5380\n",
            "1m 49s (- 43m 41s) (3000 4%) 3.7924\n",
            "1m 52s (- 43m 27s) (3100 4%) 3.4685\n",
            "1m 56s (- 43m 30s) (3200 4%) 3.9087\n",
            "1m 59s (- 43m 24s) (3300 4%) 3.4803\n",
            "2m 3s (- 43m 19s) (3400 4%) 3.4910\n",
            "2m 7s (- 43m 17s) (3500 4%) 3.5433\n",
            "2m 10s (- 43m 6s) (3600 4%) 3.5939\n",
            "2m 13s (- 42m 54s) (3700 4%) 3.6014\n",
            "2m 17s (- 42m 49s) (3800 5%) 3.5429\n",
            "2m 20s (- 42m 43s) (3900 5%) 3.5956\n",
            "2m 23s (- 42m 34s) (4000 5%) 3.6376\n",
            "2m 26s (- 42m 21s) (4100 5%) 3.4531\n",
            "2m 30s (- 42m 17s) (4200 5%) 3.6940\n",
            "2m 34s (- 42m 13s) (4300 5%) 3.5037\n",
            "2m 37s (- 42m 2s) (4400 5%) 3.5829\n",
            "2m 40s (- 41m 51s) (4500 6%) 3.3350\n",
            "2m 43s (- 41m 49s) (4600 6%) 3.6500\n",
            "2m 47s (- 41m 47s) (4700 6%) 3.4926\n",
            "2m 51s (- 41m 44s) (4800 6%) 3.3292\n",
            "2m 54s (- 41m 40s) (4900 6%) 3.3265\n",
            "2m 58s (- 41m 39s) (5000 6%) 3.6061\n",
            "3m 2s (- 41m 35s) (5100 6%) 3.4439\n",
            "3m 5s (- 41m 33s) (5200 6%) 3.4190\n",
            "3m 9s (- 41m 30s) (5300 7%) 3.4949\n",
            "3m 12s (- 41m 22s) (5400 7%) 3.3707\n",
            "3m 16s (- 41m 18s) (5500 7%) 3.4349\n",
            "3m 19s (- 41m 16s) (5600 7%) 3.4316\n",
            "3m 23s (- 41m 18s) (5700 7%) 3.4753\n",
            "3m 27s (- 41m 12s) (5800 7%) 3.4859\n",
            "3m 30s (- 41m 8s) (5900 7%) 3.3056\n",
            "3m 34s (- 41m 7s) (6000 8%) 3.5198\n",
            "3m 38s (- 41m 4s) (6100 8%) 3.4586\n",
            "3m 41s (- 41m 1s) (6200 8%) 3.3850\n",
            "3m 45s (- 40m 57s) (6300 8%) 3.2704\n",
            "3m 49s (- 40m 56s) (6400 8%) 3.5129\n",
            "3m 52s (- 40m 52s) (6500 8%) 3.3816\n",
            "3m 56s (- 40m 54s) (6600 8%) 3.2704\n",
            "4m 0s (- 40m 48s) (6700 8%) 3.5185\n",
            "4m 3s (- 40m 45s) (6800 9%) 3.1177\n",
            "4m 7s (- 40m 39s) (6900 9%) 3.3910\n",
            "4m 10s (- 40m 33s) (7000 9%) 3.3768\n",
            "4m 13s (- 40m 27s) (7100 9%) 3.2650\n",
            "4m 17s (- 40m 25s) (7200 9%) 3.3579\n",
            "4m 21s (- 40m 21s) (7300 9%) 3.2885\n",
            "4m 24s (- 40m 19s) (7400 9%) 3.2566\n",
            "4m 28s (- 40m 15s) (7500 10%) 3.2609\n",
            "4m 31s (- 40m 9s) (7600 10%) 3.2673\n",
            "4m 35s (- 40m 5s) (7700 10%) 3.2790\n",
            "4m 38s (- 39m 59s) (7800 10%) 3.1876\n",
            "4m 42s (- 39m 56s) (7900 10%) 3.3068\n",
            "4m 45s (- 39m 52s) (8000 10%) 3.3105\n",
            "4m 49s (- 39m 50s) (8100 10%) 3.5055\n",
            "4m 52s (- 39m 45s) (8200 10%) 3.2906\n",
            "4m 56s (- 39m 39s) (8300 11%) 3.2022\n",
            "4m 59s (- 39m 36s) (8400 11%) 3.3416\n",
            "5m 4s (- 39m 40s) (8500 11%) 3.3232\n",
            "5m 7s (- 39m 37s) (8600 11%) 3.2381\n",
            "5m 11s (- 39m 34s) (8700 11%) 3.2269\n",
            "5m 14s (- 39m 28s) (8800 11%) 3.0099\n",
            "5m 18s (- 39m 26s) (8900 11%) 3.2906\n",
            "5m 22s (- 39m 25s) (9000 12%) 3.1703\n",
            "5m 26s (- 39m 21s) (9100 12%) 3.0996\n",
            "5m 29s (- 39m 16s) (9200 12%) 3.2011\n",
            "5m 33s (- 39m 13s) (9300 12%) 3.2069\n",
            "5m 36s (- 39m 7s) (9400 12%) 2.9683\n",
            "5m 39s (- 39m 3s) (9500 12%) 3.2506\n",
            "5m 43s (- 38m 59s) (9600 12%) 3.1083\n",
            "5m 46s (- 38m 54s) (9700 12%) 3.2350\n",
            "5m 50s (- 38m 51s) (9800 13%) 3.2955\n",
            "5m 53s (- 38m 46s) (9900 13%) 3.2445\n",
            "5m 57s (- 38m 40s) (10000 13%) 2.8962\n",
            "6m 0s (- 38m 35s) (10100 13%) 3.1826\n",
            "6m 3s (- 38m 30s) (10200 13%) 2.9415\n",
            "6m 7s (- 38m 28s) (10300 13%) 3.1118\n",
            "6m 11s (- 38m 24s) (10400 13%) 3.0894\n",
            "6m 14s (- 38m 19s) (10500 14%) 3.1196\n",
            "6m 17s (- 38m 16s) (10600 14%) 3.1657\n",
            "6m 21s (- 38m 11s) (10700 14%) 3.1245\n",
            "6m 25s (- 38m 8s) (10800 14%) 3.1767\n",
            "6m 28s (- 38m 3s) (10900 14%) 3.1162\n",
            "6m 31s (- 37m 58s) (11000 14%) 3.1134\n",
            "6m 35s (- 37m 55s) (11100 14%) 3.0448\n",
            "6m 38s (- 37m 49s) (11200 14%) 3.2849\n",
            "6m 42s (- 37m 47s) (11300 15%) 3.0428\n",
            "6m 45s (- 37m 41s) (11400 15%) 3.1054\n",
            "6m 49s (- 37m 39s) (11500 15%) 3.1689\n",
            "6m 52s (- 37m 34s) (11600 15%) 3.1181\n",
            "6m 55s (- 37m 30s) (11700 15%) 3.2236\n",
            "6m 59s (- 37m 27s) (11800 15%) 3.1065\n",
            "7m 3s (- 37m 23s) (11900 15%) 3.0701\n",
            "7m 6s (- 37m 18s) (12000 16%) 2.9877\n",
            "7m 9s (- 37m 14s) (12100 16%) 2.9121\n",
            "7m 13s (- 37m 10s) (12200 16%) 2.9413\n",
            "7m 16s (- 37m 5s) (12300 16%) 2.9351\n",
            "7m 19s (- 37m 0s) (12400 16%) 2.9609\n",
            "7m 23s (- 36m 56s) (12500 16%) 3.0407\n",
            "7m 26s (- 36m 52s) (12600 16%) 3.0449\n",
            "7m 30s (- 36m 48s) (12700 16%) 3.0700\n",
            "7m 33s (- 36m 45s) (12800 17%) 3.0648\n",
            "7m 37s (- 36m 40s) (12900 17%) 2.8445\n",
            "7m 40s (- 36m 36s) (13000 17%) 2.9631\n",
            "7m 44s (- 36m 32s) (13100 17%) 3.0039\n",
            "7m 47s (- 36m 29s) (13200 17%) 3.0956\n",
            "7m 51s (- 36m 27s) (13300 17%) 3.1369\n",
            "7m 54s (- 36m 23s) (13400 17%) 3.1654\n",
            "7m 57s (- 36m 16s) (13500 18%) 2.9210\n",
            "8m 1s (- 36m 12s) (13600 18%) 3.1405\n",
            "8m 4s (- 36m 8s) (13700 18%) 3.0353\n",
            "8m 8s (- 36m 4s) (13800 18%) 3.0671\n",
            "8m 11s (- 36m 0s) (13900 18%) 3.1506\n",
            "8m 14s (- 35m 55s) (14000 18%) 2.9707\n",
            "8m 18s (- 35m 52s) (14100 18%) 3.0701\n",
            "8m 21s (- 35m 48s) (14200 18%) 2.9541\n",
            "8m 25s (- 35m 43s) (14300 19%) 3.0076\n",
            "8m 28s (- 35m 41s) (14400 19%) 2.9290\n",
            "8m 32s (- 35m 38s) (14500 19%) 2.8751\n",
            "8m 35s (- 35m 33s) (14600 19%) 2.9799\n",
            "8m 39s (- 35m 29s) (14700 19%) 2.9549\n",
            "8m 42s (- 35m 25s) (14800 19%) 3.0530\n",
            "8m 45s (- 35m 21s) (14900 19%) 2.8700\n",
            "8m 49s (- 35m 19s) (15000 20%) 2.8913\n",
            "8m 53s (- 35m 14s) (15100 20%) 3.0153\n",
            "8m 56s (- 35m 10s) (15200 20%) 2.9881\n",
            "9m 0s (- 35m 7s) (15300 20%) 2.8724\n",
            "9m 3s (- 35m 3s) (15400 20%) 3.0468\n",
            "9m 7s (- 35m 1s) (15500 20%) 2.9023\n",
            "9m 10s (- 34m 57s) (15600 20%) 3.0529\n",
            "9m 14s (- 34m 54s) (15700 20%) 3.0590\n",
            "9m 17s (- 34m 50s) (15800 21%) 2.8344\n",
            "9m 21s (- 34m 45s) (15900 21%) 2.7166\n",
            "9m 24s (- 34m 41s) (16000 21%) 2.7818\n",
            "9m 27s (- 34m 37s) (16100 21%) 2.7957\n",
            "9m 31s (- 34m 33s) (16200 21%) 2.6981\n",
            "9m 34s (- 34m 28s) (16300 21%) 2.7837\n",
            "9m 37s (- 34m 24s) (16400 21%) 2.9946\n",
            "9m 41s (- 34m 22s) (16500 22%) 2.8295\n",
            "9m 45s (- 34m 19s) (16600 22%) 2.7978\n",
            "9m 48s (- 34m 15s) (16700 22%) 2.9356\n",
            "9m 51s (- 34m 10s) (16800 22%) 2.7913\n",
            "9m 55s (- 34m 7s) (16900 22%) 2.7297\n",
            "9m 58s (- 34m 3s) (17000 22%) 2.8640\n",
            "10m 2s (- 33m 58s) (17100 22%) 2.7081\n",
            "10m 5s (- 33m 55s) (17200 22%) 2.6626\n",
            "10m 9s (- 33m 51s) (17300 23%) 2.8097\n",
            "10m 12s (- 33m 48s) (17400 23%) 2.9611\n",
            "10m 16s (- 33m 45s) (17500 23%) 2.9361\n",
            "10m 20s (- 33m 42s) (17600 23%) 2.7972\n",
            "10m 23s (- 33m 37s) (17700 23%) 2.9118\n",
            "10m 26s (- 33m 34s) (17800 23%) 2.9887\n",
            "10m 30s (- 33m 30s) (17900 23%) 2.8569\n",
            "10m 33s (- 33m 27s) (18000 24%) 2.9422\n",
            "10m 37s (- 33m 24s) (18100 24%) 2.9282\n",
            "10m 40s (- 33m 19s) (18200 24%) 2.9049\n",
            "10m 44s (- 33m 16s) (18300 24%) 2.8546\n",
            "10m 48s (- 33m 13s) (18400 24%) 2.8984\n",
            "10m 51s (- 33m 10s) (18500 24%) 2.8150\n",
            "10m 55s (- 33m 8s) (18600 24%) 2.6289\n",
            "10m 58s (- 33m 3s) (18700 24%) 2.8545\n",
            "11m 2s (- 33m 0s) (18800 25%) 2.8944\n",
            "11m 6s (- 32m 56s) (18900 25%) 2.6576\n",
            "11m 9s (- 32m 54s) (19000 25%) 2.7064\n",
            "11m 13s (- 32m 50s) (19100 25%) 2.8051\n",
            "11m 17s (- 32m 48s) (19200 25%) 2.6692\n",
            "11m 20s (- 32m 44s) (19300 25%) 2.7191\n",
            "11m 23s (- 32m 39s) (19400 25%) 2.7185\n",
            "11m 27s (- 32m 36s) (19500 26%) 2.8884\n",
            "11m 30s (- 32m 32s) (19600 26%) 2.6043\n",
            "11m 34s (- 32m 29s) (19700 26%) 2.6970\n",
            "11m 37s (- 32m 25s) (19800 26%) 2.8426\n",
            "11m 40s (- 32m 20s) (19900 26%) 2.7030\n",
            "11m 44s (- 32m 18s) (20000 26%) 2.6941\n",
            "11m 48s (- 32m 14s) (20100 26%) 2.8824\n",
            "11m 51s (- 32m 11s) (20200 26%) 2.6113\n",
            "11m 55s (- 32m 6s) (20300 27%) 2.6880\n",
            "11m 58s (- 32m 2s) (20400 27%) 2.6327\n",
            "12m 1s (- 31m 57s) (20500 27%) 2.7988\n",
            "12m 5s (- 31m 55s) (20600 27%) 2.7370\n",
            "12m 8s (- 31m 52s) (20700 27%) 2.7080\n",
            "12m 12s (- 31m 48s) (20800 27%) 2.5955\n",
            "12m 15s (- 31m 44s) (20900 27%) 2.7107\n",
            "12m 19s (- 31m 41s) (21000 28%) 2.8335\n",
            "12m 22s (- 31m 37s) (21100 28%) 2.8402\n",
            "12m 26s (- 31m 33s) (21200 28%) 2.6015\n",
            "12m 29s (- 31m 29s) (21300 28%) 2.6152\n",
            "12m 32s (- 31m 25s) (21400 28%) 2.8351\n",
            "12m 36s (- 31m 22s) (21500 28%) 2.6176\n",
            "12m 40s (- 31m 19s) (21600 28%) 2.7890\n",
            "12m 44s (- 31m 16s) (21700 28%) 2.6903\n",
            "12m 47s (- 31m 12s) (21800 29%) 2.6606\n",
            "12m 50s (- 31m 8s) (21900 29%) 2.5196\n",
            "12m 53s (- 31m 4s) (22000 29%) 2.4793\n",
            "12m 57s (- 31m 0s) (22100 29%) 2.7529\n",
            "13m 0s (- 30m 56s) (22200 29%) 2.4817\n",
            "13m 4s (- 30m 52s) (22300 29%) 2.5082\n",
            "13m 7s (- 30m 49s) (22400 29%) 2.5375\n",
            "13m 10s (- 30m 45s) (22500 30%) 2.5474\n",
            "13m 13s (- 30m 40s) (22600 30%) 2.5816\n",
            "13m 17s (- 30m 36s) (22700 30%) 2.7232\n",
            "13m 20s (- 30m 33s) (22800 30%) 2.4455\n",
            "13m 24s (- 30m 29s) (22900 30%) 2.5368\n",
            "13m 27s (- 30m 26s) (23000 30%) 2.6825\n",
            "13m 31s (- 30m 23s) (23100 30%) 2.4711\n",
            "13m 34s (- 30m 19s) (23200 30%) 2.6509\n",
            "13m 38s (- 30m 16s) (23300 31%) 2.6426\n",
            "13m 42s (- 30m 13s) (23400 31%) 2.7647\n",
            "13m 46s (- 30m 10s) (23500 31%) 2.6580\n",
            "13m 50s (- 30m 8s) (23600 31%) 2.6207\n",
            "13m 53s (- 30m 4s) (23700 31%) 2.4778\n",
            "13m 56s (- 30m 0s) (23800 31%) 2.5996\n",
            "14m 0s (- 29m 56s) (23900 31%) 2.5492\n",
            "14m 4s (- 29m 53s) (24000 32%) 2.6766\n",
            "14m 7s (- 29m 50s) (24100 32%) 2.6184\n",
            "14m 11s (- 29m 46s) (24200 32%) 2.5127\n",
            "14m 15s (- 29m 44s) (24300 32%) 2.7259\n",
            "14m 18s (- 29m 41s) (24400 32%) 2.6273\n",
            "14m 22s (- 29m 37s) (24500 32%) 2.5591\n",
            "14m 25s (- 29m 33s) (24600 32%) 2.5716\n",
            "14m 29s (- 29m 29s) (24700 32%) 2.3809\n",
            "14m 32s (- 29m 26s) (24800 33%) 2.6554\n",
            "14m 36s (- 29m 22s) (24900 33%) 2.6614\n",
            "14m 39s (- 29m 18s) (25000 33%) 2.3848\n",
            "14m 43s (- 29m 15s) (25100 33%) 2.5411\n",
            "14m 46s (- 29m 11s) (25200 33%) 2.6345\n",
            "14m 49s (- 29m 8s) (25300 33%) 2.4235\n",
            "14m 53s (- 29m 4s) (25400 33%) 2.2266\n",
            "14m 56s (- 29m 0s) (25500 34%) 2.2922\n",
            "15m 0s (- 28m 57s) (25600 34%) 2.5159\n",
            "15m 3s (- 28m 54s) (25700 34%) 2.4696\n",
            "15m 7s (- 28m 50s) (25800 34%) 2.4934\n",
            "15m 11s (- 28m 47s) (25900 34%) 2.3614\n",
            "15m 14s (- 28m 43s) (26000 34%) 2.3408\n",
            "15m 18s (- 28m 39s) (26100 34%) 2.4656\n",
            "15m 21s (- 28m 35s) (26200 34%) 2.5147\n",
            "15m 24s (- 28m 31s) (26300 35%) 2.4464\n",
            "15m 28s (- 28m 28s) (26400 35%) 2.4982\n",
            "15m 31s (- 28m 25s) (26500 35%) 2.4806\n",
            "15m 35s (- 28m 21s) (26600 35%) 2.5487\n",
            "15m 38s (- 28m 17s) (26700 35%) 2.5908\n",
            "15m 42s (- 28m 14s) (26800 35%) 2.3745\n",
            "15m 45s (- 28m 11s) (26900 35%) 2.5551\n",
            "15m 49s (- 28m 7s) (27000 36%) 2.3300\n",
            "15m 53s (- 28m 4s) (27100 36%) 2.6674\n",
            "15m 56s (- 28m 0s) (27200 36%) 2.3192\n",
            "15m 59s (- 27m 56s) (27300 36%) 2.7200\n",
            "16m 3s (- 27m 53s) (27400 36%) 2.6908\n",
            "16m 6s (- 27m 49s) (27500 36%) 2.4125\n",
            "16m 10s (- 27m 46s) (27600 36%) 2.4756\n",
            "16m 13s (- 27m 42s) (27700 36%) 2.4859\n",
            "16m 17s (- 27m 39s) (27800 37%) 2.4316\n",
            "16m 20s (- 27m 35s) (27900 37%) 2.5614\n",
            "16m 24s (- 27m 32s) (28000 37%) 2.3138\n",
            "16m 28s (- 27m 29s) (28100 37%) 2.4871\n",
            "16m 31s (- 27m 26s) (28200 37%) 2.3954\n",
            "16m 35s (- 27m 22s) (28300 37%) 2.5114\n",
            "16m 38s (- 27m 18s) (28400 37%) 2.2731\n",
            "16m 41s (- 27m 14s) (28500 38%) 2.4069\n",
            "16m 45s (- 27m 11s) (28600 38%) 2.3668\n",
            "16m 49s (- 27m 7s) (28700 38%) 2.4393\n",
            "16m 52s (- 27m 4s) (28800 38%) 2.4362\n",
            "16m 56s (- 27m 1s) (28900 38%) 2.6048\n",
            "16m 59s (- 26m 57s) (29000 38%) 2.2786\n",
            "17m 2s (- 26m 53s) (29100 38%) 2.0877\n",
            "17m 6s (- 26m 50s) (29200 38%) 2.3559\n",
            "17m 9s (- 26m 46s) (29300 39%) 2.3807\n",
            "17m 13s (- 26m 43s) (29400 39%) 2.4773\n",
            "17m 17s (- 26m 39s) (29500 39%) 2.3391\n",
            "17m 20s (- 26m 36s) (29600 39%) 2.5038\n",
            "17m 24s (- 26m 32s) (29700 39%) 2.1584\n",
            "17m 27s (- 26m 29s) (29800 39%) 2.6390\n",
            "17m 31s (- 26m 25s) (29900 39%) 2.5670\n",
            "17m 34s (- 26m 22s) (30000 40%) 2.3749\n",
            "17m 38s (- 26m 18s) (30100 40%) 2.5198\n",
            "17m 42s (- 26m 15s) (30200 40%) 2.3980\n",
            "17m 45s (- 26m 12s) (30300 40%) 2.3494\n",
            "17m 49s (- 26m 8s) (30400 40%) 2.1485\n",
            "17m 52s (- 26m 5s) (30500 40%) 2.4413\n",
            "17m 56s (- 26m 1s) (30600 40%) 2.3647\n",
            "17m 59s (- 25m 57s) (30700 40%) 2.2433\n",
            "18m 3s (- 25m 54s) (30800 41%) 2.3127\n",
            "18m 6s (- 25m 51s) (30900 41%) 2.3876\n",
            "18m 10s (- 25m 47s) (31000 41%) 2.3130\n",
            "18m 13s (- 25m 44s) (31100 41%) 2.2006\n",
            "18m 17s (- 25m 40s) (31200 41%) 2.2133\n",
            "18m 21s (- 25m 37s) (31300 41%) 2.3421\n",
            "18m 24s (- 25m 33s) (31400 41%) 2.1745\n",
            "18m 27s (- 25m 29s) (31500 42%) 2.2191\n",
            "18m 31s (- 25m 27s) (31600 42%) 2.4337\n",
            "18m 35s (- 25m 24s) (31700 42%) 2.3882\n",
            "18m 39s (- 25m 20s) (31800 42%) 2.3120\n",
            "18m 42s (- 25m 17s) (31900 42%) 2.2485\n",
            "18m 46s (- 25m 13s) (32000 42%) 2.3066\n",
            "18m 50s (- 25m 10s) (32100 42%) 2.4058\n",
            "18m 54s (- 25m 7s) (32200 42%) 2.3472\n",
            "18m 57s (- 25m 3s) (32300 43%) 2.3639\n",
            "19m 0s (- 24m 59s) (32400 43%) 2.0689\n",
            "19m 4s (- 24m 57s) (32500 43%) 2.3263\n",
            "19m 8s (- 24m 53s) (32600 43%) 2.3740\n",
            "19m 11s (- 24m 50s) (32700 43%) 2.0816\n",
            "19m 15s (- 24m 46s) (32800 43%) 2.1740\n",
            "19m 19s (- 24m 43s) (32900 43%) 2.1774\n",
            "19m 22s (- 24m 39s) (33000 44%) 2.1307\n",
            "19m 26s (- 24m 36s) (33100 44%) 2.3931\n",
            "19m 30s (- 24m 33s) (33200 44%) 2.1904\n",
            "19m 33s (- 24m 29s) (33300 44%) 2.1488\n",
            "19m 36s (- 24m 25s) (33400 44%) 2.1249\n",
            "19m 39s (- 24m 21s) (33500 44%) 2.1532\n",
            "19m 43s (- 24m 18s) (33600 44%) 2.0232\n",
            "19m 47s (- 24m 15s) (33700 44%) 2.0290\n",
            "19m 51s (- 24m 12s) (33800 45%) 2.2286\n",
            "19m 54s (- 24m 8s) (33900 45%) 1.9971\n",
            "19m 58s (- 24m 4s) (34000 45%) 2.2239\n",
            "20m 2s (- 24m 1s) (34100 45%) 2.0971\n",
            "20m 6s (- 23m 58s) (34200 45%) 2.1873\n",
            "20m 9s (- 23m 55s) (34300 45%) 2.2890\n",
            "20m 13s (- 23m 51s) (34400 45%) 2.4223\n",
            "20m 16s (- 23m 48s) (34500 46%) 2.2580\n",
            "20m 20s (- 23m 44s) (34600 46%) 2.1247\n",
            "20m 23s (- 23m 41s) (34700 46%) 2.1265\n",
            "20m 27s (- 23m 37s) (34800 46%) 2.0106\n",
            "20m 30s (- 23m 33s) (34900 46%) 2.0838\n",
            "20m 34s (- 23m 30s) (35000 46%) 2.1792\n",
            "20m 38s (- 23m 27s) (35100 46%) 2.0952\n",
            "20m 41s (- 23m 24s) (35200 46%) 2.1332\n",
            "20m 45s (- 23m 20s) (35300 47%) 2.0293\n",
            "20m 49s (- 23m 17s) (35400 47%) 2.1173\n",
            "20m 52s (- 23m 13s) (35500 47%) 1.9376\n",
            "20m 56s (- 23m 10s) (35600 47%) 2.1530\n",
            "20m 59s (- 23m 6s) (35700 47%) 1.9443\n",
            "21m 3s (- 23m 3s) (35800 47%) 2.2345\n",
            "21m 6s (- 22m 59s) (35900 47%) 2.0460\n",
            "21m 10s (- 22m 56s) (36000 48%) 2.0478\n",
            "21m 13s (- 22m 52s) (36100 48%) 2.0533\n",
            "21m 16s (- 22m 48s) (36200 48%) 2.0611\n",
            "21m 20s (- 22m 45s) (36300 48%) 2.0401\n",
            "21m 23s (- 22m 41s) (36400 48%) 1.8943\n",
            "21m 27s (- 22m 37s) (36500 48%) 1.9829\n",
            "21m 30s (- 22m 33s) (36600 48%) 2.2011\n",
            "21m 34s (- 22m 30s) (36700 48%) 2.0081\n",
            "21m 37s (- 22m 26s) (36800 49%) 1.9792\n",
            "21m 41s (- 22m 23s) (36900 49%) 1.7283\n",
            "21m 44s (- 22m 20s) (37000 49%) 1.9146\n",
            "21m 48s (- 22m 16s) (37100 49%) 2.1593\n",
            "21m 52s (- 22m 13s) (37200 49%) 1.9073\n",
            "21m 55s (- 22m 9s) (37300 49%) 1.9376\n",
            "21m 58s (- 22m 5s) (37400 49%) 1.9909\n",
            "22m 2s (- 22m 2s) (37500 50%) 1.9021\n",
            "22m 6s (- 21m 59s) (37600 50%) 1.9835\n",
            "22m 9s (- 21m 55s) (37700 50%) 1.8317\n",
            "22m 13s (- 21m 52s) (37800 50%) 1.9183\n",
            "22m 16s (- 21m 48s) (37900 50%) 2.0195\n",
            "22m 20s (- 21m 45s) (38000 50%) 2.0622\n",
            "22m 23s (- 21m 41s) (38100 50%) 2.0036\n",
            "22m 27s (- 21m 38s) (38200 50%) 1.7692\n",
            "22m 31s (- 21m 34s) (38300 51%) 1.8295\n",
            "22m 34s (- 21m 31s) (38400 51%) 1.9051\n",
            "22m 38s (- 21m 27s) (38500 51%) 1.9249\n",
            "22m 42s (- 21m 24s) (38600 51%) 2.1272\n",
            "22m 45s (- 21m 20s) (38700 51%) 2.1470\n",
            "22m 49s (- 21m 17s) (38800 51%) 1.7636\n",
            "22m 52s (- 21m 13s) (38900 51%) 1.6122\n",
            "22m 56s (- 21m 10s) (39000 52%) 1.8344\n",
            "22m 59s (- 21m 6s) (39100 52%) 1.7920\n",
            "23m 3s (- 21m 3s) (39200 52%) 2.0083\n",
            "23m 6s (- 20m 59s) (39300 52%) 1.8342\n",
            "23m 10s (- 20m 56s) (39400 52%) 2.0293\n",
            "23m 13s (- 20m 52s) (39500 52%) 1.9281\n",
            "23m 17s (- 20m 48s) (39600 52%) 1.6931\n",
            "23m 20s (- 20m 45s) (39700 52%) 1.8300\n",
            "23m 24s (- 20m 41s) (39800 53%) 1.7384\n",
            "23m 28s (- 20m 38s) (39900 53%) 1.7712\n",
            "23m 31s (- 20m 35s) (40000 53%) 2.0046\n",
            "23m 34s (- 20m 31s) (40100 53%) 1.7877\n",
            "23m 38s (- 20m 27s) (40200 53%) 1.9980\n",
            "23m 42s (- 20m 24s) (40300 53%) 1.8266\n",
            "23m 45s (- 20m 21s) (40400 53%) 1.8998\n",
            "23m 49s (- 20m 17s) (40500 54%) 1.9181\n",
            "23m 52s (- 20m 13s) (40600 54%) 1.8333\n",
            "23m 56s (- 20m 10s) (40700 54%) 1.9143\n",
            "24m 0s (- 20m 7s) (40800 54%) 1.9074\n",
            "24m 4s (- 20m 4s) (40900 54%) 1.8974\n",
            "24m 8s (- 20m 0s) (41000 54%) 1.8238\n",
            "24m 11s (- 19m 57s) (41100 54%) 1.7282\n",
            "24m 15s (- 19m 53s) (41200 54%) 1.6809\n",
            "24m 18s (- 19m 50s) (41300 55%) 1.7614\n",
            "24m 22s (- 19m 46s) (41400 55%) 1.8542\n",
            "24m 26s (- 19m 43s) (41500 55%) 1.9940\n",
            "24m 29s (- 19m 39s) (41600 55%) 1.6569\n",
            "24m 33s (- 19m 36s) (41700 55%) 1.8757\n",
            "24m 36s (- 19m 32s) (41800 55%) 1.9787\n",
            "24m 39s (- 19m 28s) (41900 55%) 1.7515\n",
            "24m 43s (- 19m 25s) (42000 56%) 1.7352\n",
            "24m 47s (- 19m 22s) (42100 56%) 1.7853\n",
            "24m 51s (- 19m 19s) (42200 56%) 1.8659\n",
            "24m 54s (- 19m 15s) (42300 56%) 1.7534\n",
            "24m 58s (- 19m 12s) (42400 56%) 1.7265\n",
            "25m 2s (- 19m 8s) (42500 56%) 1.7315\n",
            "25m 5s (- 19m 5s) (42600 56%) 1.6588\n",
            "25m 9s (- 19m 1s) (42700 56%) 1.7078\n",
            "25m 12s (- 18m 58s) (42800 57%) 1.8097\n",
            "25m 16s (- 18m 54s) (42900 57%) 1.7310\n",
            "25m 19s (- 18m 50s) (43000 57%) 1.7833\n",
            "25m 23s (- 18m 47s) (43100 57%) 1.7123\n",
            "25m 26s (- 18m 43s) (43200 57%) 1.7742\n",
            "25m 29s (- 18m 40s) (43300 57%) 1.7527\n",
            "25m 33s (- 18m 36s) (43400 57%) 1.8308\n",
            "25m 37s (- 18m 33s) (43500 57%) 1.6733\n",
            "25m 41s (- 18m 29s) (43600 58%) 1.7558\n",
            "25m 44s (- 18m 26s) (43700 58%) 2.0687\n",
            "25m 48s (- 18m 23s) (43800 58%) 1.6486\n",
            "25m 52s (- 18m 19s) (43900 58%) 1.6249\n",
            "25m 55s (- 18m 15s) (44000 58%) 1.9754\n",
            "25m 59s (- 18m 12s) (44100 58%) 1.7800\n",
            "26m 3s (- 18m 9s) (44200 58%) 1.6763\n",
            "26m 7s (- 18m 5s) (44300 59%) 1.7935\n",
            "26m 10s (- 18m 2s) (44400 59%) 1.7507\n",
            "26m 14s (- 17m 59s) (44500 59%) 1.7977\n",
            "26m 18s (- 17m 55s) (44600 59%) 1.9075\n",
            "26m 21s (- 17m 52s) (44700 59%) 1.7390\n",
            "26m 25s (- 17m 48s) (44800 59%) 1.7812\n",
            "26m 28s (- 17m 45s) (44900 59%) 1.7346\n",
            "26m 32s (- 17m 41s) (45000 60%) 1.6068\n",
            "26m 35s (- 17m 38s) (45100 60%) 1.7291\n",
            "26m 39s (- 17m 34s) (45200 60%) 1.6781\n",
            "26m 42s (- 17m 30s) (45300 60%) 1.5707\n",
            "26m 46s (- 17m 27s) (45400 60%) 1.5146\n",
            "26m 50s (- 17m 24s) (45500 60%) 1.8120\n",
            "26m 53s (- 17m 20s) (45600 60%) 1.6563\n",
            "26m 56s (- 17m 16s) (45700 60%) 1.7882\n",
            "27m 0s (- 17m 13s) (45800 61%) 1.7139\n",
            "27m 4s (- 17m 9s) (45900 61%) 1.6040\n",
            "27m 7s (- 17m 6s) (46000 61%) 1.4930\n",
            "27m 11s (- 17m 2s) (46100 61%) 1.4618\n",
            "27m 15s (- 16m 59s) (46200 61%) 1.5565\n",
            "27m 19s (- 16m 56s) (46300 61%) 1.5555\n",
            "27m 22s (- 16m 52s) (46400 61%) 1.7181\n",
            "27m 26s (- 16m 49s) (46500 62%) 1.5294\n",
            "27m 30s (- 16m 45s) (46600 62%) 1.7266\n",
            "27m 33s (- 16m 42s) (46700 62%) 1.5901\n",
            "27m 37s (- 16m 38s) (46800 62%) 1.5341\n",
            "27m 40s (- 16m 35s) (46900 62%) 1.7574\n",
            "27m 44s (- 16m 31s) (47000 62%) 1.6911\n",
            "27m 47s (- 16m 28s) (47100 62%) 1.6390\n",
            "27m 51s (- 16m 24s) (47200 62%) 1.6662\n",
            "27m 55s (- 16m 21s) (47300 63%) 1.7455\n",
            "27m 58s (- 16m 17s) (47400 63%) 1.5667\n",
            "28m 2s (- 16m 13s) (47500 63%) 1.5647\n",
            "28m 6s (- 16m 10s) (47600 63%) 1.5895\n",
            "28m 9s (- 16m 7s) (47700 63%) 1.5588\n",
            "28m 13s (- 16m 3s) (47800 63%) 1.6155\n",
            "28m 16s (- 15m 59s) (47900 63%) 1.5825\n",
            "28m 20s (- 15m 56s) (48000 64%) 1.5212\n",
            "28m 24s (- 15m 52s) (48100 64%) 1.7390\n",
            "28m 27s (- 15m 49s) (48200 64%) 1.4997\n",
            "28m 31s (- 15m 45s) (48300 64%) 1.6061\n",
            "28m 34s (- 15m 42s) (48400 64%) 1.5130\n",
            "28m 38s (- 15m 38s) (48500 64%) 1.6462\n",
            "28m 41s (- 15m 35s) (48600 64%) 1.4886\n",
            "28m 45s (- 15m 31s) (48700 64%) 1.6160\n",
            "28m 48s (- 15m 28s) (48800 65%) 1.3872\n",
            "28m 52s (- 15m 24s) (48900 65%) 1.5246\n",
            "28m 55s (- 15m 20s) (49000 65%) 1.3170\n",
            "28m 59s (- 15m 17s) (49100 65%) 1.4587\n",
            "29m 2s (- 15m 13s) (49200 65%) 1.3757\n",
            "29m 6s (- 15m 10s) (49300 65%) 1.4909\n",
            "29m 10s (- 15m 7s) (49400 65%) 1.6372\n",
            "29m 13s (- 15m 3s) (49500 66%) 1.3382\n",
            "29m 17s (- 14m 59s) (49600 66%) 1.6377\n",
            "29m 21s (- 14m 56s) (49700 66%) 1.6383\n",
            "29m 24s (- 14m 52s) (49800 66%) 1.6608\n",
            "29m 28s (- 14m 49s) (49900 66%) 1.4594\n",
            "29m 32s (- 14m 46s) (50000 66%) 1.6255\n",
            "29m 35s (- 14m 42s) (50100 66%) 1.6101\n",
            "29m 38s (- 14m 38s) (50200 66%) 1.6601\n",
            "29m 42s (- 14m 35s) (50300 67%) 1.4483\n",
            "29m 46s (- 14m 32s) (50400 67%) 1.5747\n",
            "29m 50s (- 14m 28s) (50500 67%) 1.6793\n",
            "29m 54s (- 14m 25s) (50600 67%) 1.5000\n",
            "29m 57s (- 14m 21s) (50700 67%) 1.5055\n",
            "30m 1s (- 14m 18s) (50800 67%) 1.5181\n",
            "30m 5s (- 14m 14s) (50900 67%) 1.5320\n",
            "30m 8s (- 14m 11s) (51000 68%) 1.3387\n",
            "30m 12s (- 14m 7s) (51100 68%) 1.6663\n",
            "30m 16s (- 14m 4s) (51200 68%) 1.4439\n",
            "30m 19s (- 14m 0s) (51300 68%) 1.6078\n",
            "30m 23s (- 13m 57s) (51400 68%) 1.5790\n",
            "30m 26s (- 13m 53s) (51500 68%) 1.5313\n",
            "30m 30s (- 13m 50s) (51600 68%) 1.5583\n",
            "30m 34s (- 13m 46s) (51700 68%) 1.3800\n",
            "30m 37s (- 13m 43s) (51800 69%) 1.4362\n",
            "30m 41s (- 13m 39s) (51900 69%) 1.4156\n",
            "30m 45s (- 13m 36s) (52000 69%) 1.7552\n",
            "30m 50s (- 13m 33s) (52100 69%) 1.4355\n",
            "30m 54s (- 13m 29s) (52200 69%) 1.4740\n",
            "30m 57s (- 13m 26s) (52300 69%) 1.4396\n",
            "31m 0s (- 13m 22s) (52400 69%) 1.3850\n",
            "31m 4s (- 13m 19s) (52500 70%) 1.3468\n",
            "31m 8s (- 13m 15s) (52600 70%) 1.6620\n",
            "31m 12s (- 13m 12s) (52700 70%) 1.3756\n",
            "31m 15s (- 13m 8s) (52800 70%) 1.5217\n",
            "31m 19s (- 13m 4s) (52900 70%) 1.3532\n",
            "31m 22s (- 13m 1s) (53000 70%) 1.4730\n",
            "31m 25s (- 12m 57s) (53100 70%) 1.3819\n",
            "31m 29s (- 12m 54s) (53200 70%) 1.5249\n",
            "31m 32s (- 12m 50s) (53300 71%) 1.3615\n",
            "31m 36s (- 12m 47s) (53400 71%) 1.6388\n",
            "31m 40s (- 12m 43s) (53500 71%) 1.5147\n",
            "31m 44s (- 12m 40s) (53600 71%) 1.3268\n",
            "31m 48s (- 12m 37s) (53700 71%) 1.2962\n",
            "31m 52s (- 12m 33s) (53800 71%) 1.5185\n",
            "31m 56s (- 12m 30s) (53900 71%) 1.4732\n",
            "32m 0s (- 12m 26s) (54000 72%) 1.5645\n",
            "32m 4s (- 12m 23s) (54100 72%) 1.4236\n",
            "32m 8s (- 12m 19s) (54200 72%) 1.5051\n",
            "32m 12s (- 12m 16s) (54300 72%) 1.5780\n",
            "32m 15s (- 12m 13s) (54400 72%) 1.4316\n",
            "32m 19s (- 12m 9s) (54500 72%) 1.2844\n",
            "32m 23s (- 12m 6s) (54600 72%) 1.5616\n",
            "32m 27s (- 12m 2s) (54700 72%) 1.2558\n",
            "32m 30s (- 11m 59s) (54800 73%) 1.3781\n",
            "32m 34s (- 11m 55s) (54900 73%) 1.4832\n",
            "32m 38s (- 11m 52s) (55000 73%) 1.3904\n",
            "32m 42s (- 11m 48s) (55100 73%) 1.4761\n",
            "32m 46s (- 11m 45s) (55200 73%) 1.3917\n",
            "32m 50s (- 11m 42s) (55300 73%) 1.4859\n",
            "32m 54s (- 11m 38s) (55400 73%) 1.3070\n",
            "32m 58s (- 11m 35s) (55500 74%) 1.5757\n",
            "33m 2s (- 11m 31s) (55600 74%) 1.5654\n",
            "33m 6s (- 11m 28s) (55700 74%) 1.4737\n",
            "33m 10s (- 11m 24s) (55800 74%) 1.3782\n",
            "33m 14s (- 11m 21s) (55900 74%) 1.2096\n",
            "33m 18s (- 11m 17s) (56000 74%) 1.3104\n",
            "33m 22s (- 11m 14s) (56100 74%) 1.2819\n",
            "33m 25s (- 11m 11s) (56200 74%) 1.3302\n",
            "33m 29s (- 11m 7s) (56300 75%) 1.1193\n",
            "33m 33s (- 11m 3s) (56400 75%) 1.4912\n",
            "33m 37s (- 11m 0s) (56500 75%) 1.4276\n",
            "33m 40s (- 10m 56s) (56600 75%) 1.2198\n",
            "33m 44s (- 10m 53s) (56700 75%) 1.5067\n",
            "33m 48s (- 10m 49s) (56800 75%) 1.1321\n",
            "33m 52s (- 10m 46s) (56900 75%) 1.1303\n",
            "33m 55s (- 10m 42s) (57000 76%) 1.3397\n",
            "33m 59s (- 10m 39s) (57100 76%) 1.4573\n",
            "34m 3s (- 10m 35s) (57200 76%) 1.5663\n",
            "34m 7s (- 10m 32s) (57300 76%) 1.2397\n",
            "34m 10s (- 10m 28s) (57400 76%) 1.3972\n",
            "34m 14s (- 10m 25s) (57500 76%) 1.4156\n",
            "34m 17s (- 10m 21s) (57600 76%) 1.3028\n",
            "34m 21s (- 10m 18s) (57700 76%) 1.3452\n",
            "34m 24s (- 10m 14s) (57800 77%) 1.4081\n",
            "34m 28s (- 10m 10s) (57900 77%) 1.2961\n",
            "34m 32s (- 10m 7s) (58000 77%) 1.2842\n",
            "34m 36s (- 10m 3s) (58100 77%) 1.3334\n",
            "34m 39s (- 10m 0s) (58200 77%) 1.3985\n",
            "34m 43s (- 9m 56s) (58300 77%) 1.1589\n",
            "34m 47s (- 9m 53s) (58400 77%) 1.4043\n",
            "34m 51s (- 9m 49s) (58500 78%) 1.2867\n",
            "34m 54s (- 9m 46s) (58600 78%) 1.2768\n",
            "34m 58s (- 9m 42s) (58700 78%) 1.3066\n",
            "35m 1s (- 9m 39s) (58800 78%) 1.3541\n",
            "35m 6s (- 9m 35s) (58900 78%) 1.3072\n",
            "35m 9s (- 9m 32s) (59000 78%) 1.2966\n",
            "35m 13s (- 9m 28s) (59100 78%) 1.3789\n",
            "35m 16s (- 9m 25s) (59200 78%) 1.4797\n",
            "35m 20s (- 9m 21s) (59300 79%) 1.5519\n",
            "35m 24s (- 9m 17s) (59400 79%) 1.4432\n",
            "35m 28s (- 9m 14s) (59500 79%) 1.5254\n",
            "35m 31s (- 9m 10s) (59600 79%) 1.3445\n",
            "35m 34s (- 9m 7s) (59700 79%) 1.2166\n",
            "35m 38s (- 9m 3s) (59800 79%) 1.5211\n",
            "35m 42s (- 8m 59s) (59900 79%) 1.5150\n",
            "35m 46s (- 8m 56s) (60000 80%) 1.3256\n",
            "35m 49s (- 8m 52s) (60100 80%) 1.2101\n",
            "35m 53s (- 8m 49s) (60200 80%) 1.3388\n",
            "35m 57s (- 8m 45s) (60300 80%) 1.0547\n",
            "36m 0s (- 8m 42s) (60400 80%) 1.4477\n",
            "36m 4s (- 8m 38s) (60500 80%) 1.2292\n",
            "36m 8s (- 8m 35s) (60600 80%) 1.1628\n",
            "36m 11s (- 8m 31s) (60700 80%) 1.2789\n",
            "36m 15s (- 8m 28s) (60800 81%) 1.3721\n",
            "36m 18s (- 8m 24s) (60900 81%) 1.2085\n",
            "36m 22s (- 8m 20s) (61000 81%) 1.2862\n",
            "36m 26s (- 8m 17s) (61100 81%) 1.0509\n",
            "36m 29s (- 8m 13s) (61200 81%) 1.3695\n",
            "36m 32s (- 8m 10s) (61300 81%) 1.3772\n",
            "36m 36s (- 8m 6s) (61400 81%) 1.1772\n",
            "36m 40s (- 8m 2s) (61500 82%) 1.3312\n",
            "36m 43s (- 7m 59s) (61600 82%) 1.1614\n",
            "36m 47s (- 7m 55s) (61700 82%) 1.2421\n",
            "36m 51s (- 7m 52s) (61800 82%) 1.1816\n",
            "36m 54s (- 7m 48s) (61900 82%) 1.1709\n",
            "36m 58s (- 7m 45s) (62000 82%) 1.3788\n",
            "37m 2s (- 7m 41s) (62100 82%) 1.4305\n",
            "37m 6s (- 7m 38s) (62200 82%) 1.2770\n",
            "37m 9s (- 7m 34s) (62300 83%) 1.2719\n",
            "37m 13s (- 7m 30s) (62400 83%) 1.1934\n",
            "37m 17s (- 7m 27s) (62500 83%) 1.2856\n",
            "37m 20s (- 7m 23s) (62600 83%) 1.2711\n",
            "37m 23s (- 7m 20s) (62700 83%) 1.0615\n",
            "37m 27s (- 7m 16s) (62800 83%) 1.2424\n",
            "37m 30s (- 7m 12s) (62900 83%) 1.2445\n",
            "37m 33s (- 7m 9s) (63000 84%) 1.1894\n",
            "37m 37s (- 7m 5s) (63100 84%) 1.2455\n",
            "37m 41s (- 7m 2s) (63200 84%) 1.3131\n",
            "37m 45s (- 6m 58s) (63300 84%) 1.2449\n",
            "37m 49s (- 6m 55s) (63400 84%) 1.2131\n",
            "37m 53s (- 6m 51s) (63500 84%) 0.9847\n",
            "37m 57s (- 6m 48s) (63600 84%) 1.3472\n",
            "38m 1s (- 6m 44s) (63700 84%) 1.3558\n",
            "38m 5s (- 6m 41s) (63800 85%) 1.0513\n",
            "38m 8s (- 6m 37s) (63900 85%) 1.2572\n",
            "38m 12s (- 6m 34s) (64000 85%) 1.2641\n",
            "38m 16s (- 6m 30s) (64100 85%) 1.1588\n",
            "38m 20s (- 6m 26s) (64200 85%) 1.0191\n",
            "38m 23s (- 6m 23s) (64300 85%) 1.0024\n",
            "38m 27s (- 6m 19s) (64400 85%) 1.2133\n",
            "38m 31s (- 6m 16s) (64500 86%) 1.4476\n",
            "38m 35s (- 6m 12s) (64600 86%) 1.1180\n",
            "38m 39s (- 6m 9s) (64700 86%) 1.4171\n",
            "38m 42s (- 6m 5s) (64800 86%) 1.3394\n",
            "38m 46s (- 6m 2s) (64900 86%) 1.0727\n",
            "38m 50s (- 5m 58s) (65000 86%) 1.2284\n",
            "38m 54s (- 5m 54s) (65100 86%) 1.2738\n",
            "38m 57s (- 5m 51s) (65200 86%) 1.2397\n",
            "39m 1s (- 5m 47s) (65300 87%) 1.1337\n",
            "39m 5s (- 5m 44s) (65400 87%) 1.4050\n",
            "39m 9s (- 5m 40s) (65500 87%) 1.2858\n",
            "39m 13s (- 5m 37s) (65600 87%) 1.0653\n",
            "39m 16s (- 5m 33s) (65700 87%) 1.2863\n",
            "39m 20s (- 5m 30s) (65800 87%) 1.2603\n",
            "39m 24s (- 5m 26s) (65900 87%) 0.9876\n",
            "39m 28s (- 5m 22s) (66000 88%) 1.1213\n",
            "39m 32s (- 5m 19s) (66100 88%) 1.1799\n",
            "39m 35s (- 5m 15s) (66200 88%) 1.0209\n",
            "39m 39s (- 5m 12s) (66300 88%) 1.3138\n",
            "39m 43s (- 5m 8s) (66400 88%) 1.0165\n",
            "39m 47s (- 5m 5s) (66500 88%) 1.2373\n",
            "39m 50s (- 5m 1s) (66600 88%) 1.0413\n",
            "39m 54s (- 4m 57s) (66700 88%) 1.2112\n",
            "39m 58s (- 4m 54s) (66800 89%) 1.2264\n",
            "40m 2s (- 4m 50s) (66900 89%) 1.3100\n",
            "40m 6s (- 4m 47s) (67000 89%) 1.1154\n",
            "40m 10s (- 4m 43s) (67100 89%) 1.1525\n",
            "40m 13s (- 4m 40s) (67200 89%) 1.1487\n",
            "40m 17s (- 4m 36s) (67300 89%) 1.1686\n",
            "40m 21s (- 4m 32s) (67400 89%) 1.2801\n",
            "40m 24s (- 4m 29s) (67500 90%) 1.1236\n",
            "40m 28s (- 4m 25s) (67600 90%) 1.0530\n",
            "40m 31s (- 4m 22s) (67700 90%) 1.2540\n",
            "40m 35s (- 4m 18s) (67800 90%) 1.1081\n",
            "40m 39s (- 4m 15s) (67900 90%) 1.1420\n",
            "40m 43s (- 4m 11s) (68000 90%) 1.2366\n",
            "40m 46s (- 4m 7s) (68100 90%) 1.2720\n",
            "40m 50s (- 4m 4s) (68200 90%) 1.3606\n",
            "40m 54s (- 4m 0s) (68300 91%) 1.2837\n",
            "40m 57s (- 3m 57s) (68400 91%) 1.2400\n",
            "41m 1s (- 3m 53s) (68500 91%) 1.2071\n",
            "41m 6s (- 3m 50s) (68600 91%) 1.2059\n",
            "41m 9s (- 3m 46s) (68700 91%) 1.2438\n",
            "41m 13s (- 3m 42s) (68800 91%) 1.2807\n",
            "41m 17s (- 3m 39s) (68900 91%) 1.2125\n",
            "41m 20s (- 3m 35s) (69000 92%) 1.0376\n",
            "41m 24s (- 3m 32s) (69100 92%) 1.3534\n",
            "41m 27s (- 3m 28s) (69200 92%) 1.2524\n",
            "41m 31s (- 3m 24s) (69300 92%) 1.1604\n",
            "41m 34s (- 3m 21s) (69400 92%) 1.2891\n",
            "41m 38s (- 3m 17s) (69500 92%) 1.5297\n",
            "41m 42s (- 3m 14s) (69600 92%) 1.2217\n",
            "41m 45s (- 3m 10s) (69700 92%) 1.2006\n",
            "41m 49s (- 3m 6s) (69800 93%) 1.3692\n",
            "41m 53s (- 3m 3s) (69900 93%) 1.3386\n",
            "41m 57s (- 2m 59s) (70000 93%) 1.4133\n",
            "42m 0s (- 2m 56s) (70100 93%) 1.2377\n",
            "42m 4s (- 2m 52s) (70200 93%) 1.3678\n",
            "42m 8s (- 2m 49s) (70300 93%) 1.3152\n",
            "42m 11s (- 2m 45s) (70400 93%) 1.3311\n",
            "42m 15s (- 2m 41s) (70500 94%) 1.1280\n",
            "42m 18s (- 2m 38s) (70600 94%) 1.2621\n",
            "42m 22s (- 2m 34s) (70700 94%) 1.3795\n",
            "42m 25s (- 2m 31s) (70800 94%) 1.1986\n",
            "42m 28s (- 2m 27s) (70900 94%) 1.1383\n",
            "42m 32s (- 2m 23s) (71000 94%) 1.3422\n",
            "42m 36s (- 2m 20s) (71100 94%) 1.2513\n",
            "42m 39s (- 2m 16s) (71200 94%) 1.3389\n",
            "42m 43s (- 2m 13s) (71300 95%) 1.2569\n",
            "42m 46s (- 2m 9s) (71400 95%) 1.4415\n",
            "42m 50s (- 2m 5s) (71500 95%) 1.2752\n",
            "42m 53s (- 2m 2s) (71600 95%) 1.1279\n",
            "42m 57s (- 1m 58s) (71700 95%) 1.3077\n",
            "43m 1s (- 1m 55s) (71800 95%) 1.2842\n",
            "43m 5s (- 1m 51s) (71900 95%) 1.2718\n",
            "43m 8s (- 1m 47s) (72000 96%) 1.3039\n",
            "43m 12s (- 1m 44s) (72100 96%) 1.3412\n",
            "43m 16s (- 1m 40s) (72200 96%) 1.2790\n",
            "43m 19s (- 1m 37s) (72300 96%) 1.0937\n",
            "43m 23s (- 1m 33s) (72400 96%) 1.2707\n",
            "43m 27s (- 1m 29s) (72500 96%) 1.2799\n",
            "43m 31s (- 1m 26s) (72600 96%) 1.2727\n",
            "43m 34s (- 1m 22s) (72700 96%) 1.1283\n",
            "43m 38s (- 1m 19s) (72800 97%) 1.2959\n",
            "43m 41s (- 1m 15s) (72900 97%) 1.1604\n",
            "43m 45s (- 1m 11s) (73000 97%) 1.4350\n",
            "43m 49s (- 1m 8s) (73100 97%) 1.3315\n",
            "43m 52s (- 1m 4s) (73200 97%) 1.4301\n",
            "43m 56s (- 1m 1s) (73300 97%) 1.3195\n",
            "43m 59s (- 0m 57s) (73400 97%) 1.3893\n",
            "44m 3s (- 0m 53s) (73500 98%) 1.4782\n",
            "44m 7s (- 0m 50s) (73600 98%) 1.3310\n",
            "44m 10s (- 0m 46s) (73700 98%) 1.2434\n",
            "44m 14s (- 0m 43s) (73800 98%) 1.3478\n",
            "44m 17s (- 0m 39s) (73900 98%) 1.3781\n",
            "44m 21s (- 0m 35s) (74000 98%) 1.3196\n",
            "44m 24s (- 0m 32s) (74100 98%) 1.2150\n",
            "44m 28s (- 0m 28s) (74200 98%) 1.3038\n",
            "44m 31s (- 0m 25s) (74300 99%) 1.3308\n",
            "44m 35s (- 0m 21s) (74400 99%) 1.2550\n",
            "44m 38s (- 0m 17s) (74500 99%) 1.4853\n",
            "44m 42s (- 0m 14s) (74600 99%) 1.3370\n",
            "44m 46s (- 0m 10s) (74700 99%) 1.2036\n",
            "44m 50s (- 0m 7s) (74800 99%) 1.3248\n",
            "44m 53s (- 0m 3s) (74900 99%) 1.0337\n",
            "44m 57s (- 0m 0s) (75000 100%) 1.2411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPenhx17yucP",
        "colab_type": "code",
        "outputId": "f8e668e3-d751-483b-d4b8-b70327350f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "showPlot(train_losses)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD6CAYAAACxrrxPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9u3eQslGxTW",
        "colab_type": "code",
        "outputId": "c26fdbd5-063c-4b12-d8a4-2f50f31c495b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "# Train evaluation\n",
        "evaluateRandomly(encoder1, attn_decoder1, pairs)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> create a dictionary `list_dict` containing each tuple in list `tuple_list` as values and the tuple's first element as the corresponding key\n",
            "= list_dict = {t[0]: t for t in tuple_list}\n",
            "< list( {x.[[x]:.)}<EOS>\n",
            "\n",
            "> create list `changed_list ` containing elements of list `original_list` whilst converting strings containing digits to integers\n",
            "= changed_list = [(int(f) if f.isdigit() else f) for f in original_list]\n",
            "< changed_list = [(int(f) if f.isdigit() else f])<EOS>\n",
            "\n",
            "> Check the status code of url `url`\n",
            "= r = requests.head(url)\n",
            "return (r.status_code == 200)\n",
            "< r = requests.head(url)<EOS>\n",
            "\n",
            "> SQLAlchemy select records of columns of table `my_table` in addition to current date column\n",
            "= print(select([my_table, func.current_date()]).execute())\n",
            "< print(select([my_table, func.current_date())]<EOS>\n",
            "\n",
            "> Get a list from two strings `12345` and `ab` with values as each character concatenated\n",
            "= [(x + y) for x in '12345' for y in 'ab']\n",
            "< [(x + y) for x in '12345' for y in 'ab']<EOS>\n",
            "\n",
            "> make a window `root` jump to the front\n",
            "= root.attributes('-topmost', True)\n",
            "< root.lift()-)<EOS>\n",
            "\n",
            "> Format a string `u'Andr\\xc3\\xa9'` that has unicode characters\n",
            "= \"\"\"\"\"\".join(chr(ord(c)) for c in 'Andr\\xc3\\xa9')\n",
            "< \"\"\"\"\"\"\".join(chr(ord(ord(c) for c in 'Andr\\xc3\\xa9')<EOS>\n",
            "\n",
            "> Get Last Day of the second month in 2002\n",
            "= calendar.monthrange(2008, 2)\n",
            "< calendar.monthrange(2008, 2)<EOS>\n",
            "\n",
            "> sort a multidimensional array `a` by column with index 1\n",
            "= sorted(a, key=lambda x: x[1])\n",
            "< sorted(a, key=lambda x: x(1))<EOS>\n",
            "\n",
            "> get number in list `myList` closest in value to number `myNumber`\n",
            "= min(myList, key=lambda x: abs(x - myNumber))\n",
            "< min(myList, key=lambda x)((x - myNumber* 2))<EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_qecZTVv2m0",
        "colab_type": "code",
        "outputId": "012b6d7d-e47e-4ba7-a141-349e9224c3fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_answers = evaluate_data_set(encoder1,attn_decoder1, pairs)\n",
        "acc = 0\n",
        "# Compute how many full code line is the same as the data\n",
        "for i in range(len(train_answers)):\n",
        "  if pairs[i][1] == train_answers[i]:\n",
        "    acc+=1\n",
        "print(\"Correct on {} snippets out of {} on training set\".format(acc,len(train_answers)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Concatenate elements of a list 'x' of multiple integers to a single integer\n",
            "= sum(d * 10 ** i for i, d in enumerate(x[::-1]))\n",
            "< sum(d * 10 ** i for i, d in enumerate(x[::-1]))\n",
            "\n",
            "> execute os command ''TASKKILL /F /IM firefox.exe''\n",
            "= os.system('TASKKILL /F /IM firefox.exe')\n",
            "< os.system('TASKKILL /F /IM firefox.exe')\n",
            "\n",
            "> trim whitespace in string `s`\n",
            "= s.strip()\n",
            "< s.strip()\n",
            "\n",
            "> calculate the sum of the squares of each value in list `l`\n",
            "= sum(map(lambda x: x * x, l))\n",
            "< sum(map(lambda x: x * x, x[0]), l)\n",
            "\n",
            "> sort list `lst` in descending order based on the second item of each tuple in it\n",
            "= lst.sort(key=lambda x: x[2], reverse=True)\n",
            "< lst.sort(key=lambda x: x[2]), x[1], x[0])\n",
            "\n",
            "> check if any item from list `b` is in list `a`\n",
            "= print(any(x in a for x in b))\n",
            "< print(any(x in a for x in b))\n",
            "\n",
            "> convert string `x'  to dictionary splitted by `=` using list comprehension\n",
            "= dict([x.split('=') for x in s.split()])\n",
            "< dict([xsplit='split''[split('split',split)))\n",
            "\n",
            "> lowercase string values with key 'content' in a list of dictionaries `messages`\n",
            "= [{'content': x['content'].lower()} for x in messages]\n",
            "< [{'content': x['content'] 'content']})\n",
            "\n",
            "> sort a pandas data frame by column `Peak` in ascending and `Weeks` in descending order\n",
            "= df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n",
            "< df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n",
            "\n",
            "> calculate the date six months from the current date\n",
            "= print((datetime.date.today() + datetime.timedelta(((6 * 365) / 12))).isoformat())\n",
            "< print((datetime..(((today(today()((()((()((())(\n",
            "\n",
            "> parse a YAML file \"example.yaml\"\n",
            "= with open('example.yaml') as stream:\n",
            "    try:\n",
            "        print((yaml.load(stream)))\n",
            "    except yaml.YAMLError as exc:\n",
            "        print(exc)\n",
            "< with open('example.yaml')\n",
            "\n",
            "> sort rows of numpy matrix `arr` in ascending order according to all column values\n",
            "= numpy.sort(arr, axis=0)\n",
            "< numpy.sort(arr, axis=0)\n",
            "\n",
            "> get set intersection between dictionaries `d1` and `d2`\n",
            "= dict((x, set(y) & set(d1.get(x, ()))) for x, y in d2.items())\n",
            "< print((x, set(y) == )((x, d))(d1.items())))\n",
            "\n",
            "> get a list of locally installed Python modules\n",
            "= help('modules')\n",
            "< help('modules',\n",
            "\n",
            "> Open file 'sample.json' in read mode with encoding of 'utf-8-sig'\n",
            "= json.load(codecs.open('sample.json', 'r', 'utf-8-sig'))\n",
            "< json.load(codecs.open('sample.json', 'r'))\n",
            "\n",
            "> convert numpy array into python list structure\n",
            "= np.array([[1, 2, 3], [4, 5, 6]]).tolist()\n",
            "< np.array([,1,,,,,,,,,,,,,,,,,,,,,,,\n",
            "\n",
            "> find consecutive consonants in a word `CONCENTRATION` using regex\n",
            "= re.findall('[bcdfghjklmnpqrstvwxyz]+', 'CONCERTATION', re.IGNORECASE)\n",
            "< re.findall('[bcdfghjklmnpqrstvwxyz]+?,?|=', ', text.',\n",
            "\n",
            "> make a flat list from list of lists `list2d`\n",
            "= list(itertools.chain.from_iterable(list2d))\n",
            "< list(itertools.chain.from_iterable(list2d))\n",
            "\n",
            "> Fit Kmeans function to a one-dimensional array `x` by reshaping it to be a multidimensional array of single values\n",
            "= km.fit(x.reshape(-1, 1))\n",
            "< km.fit('.reshape--,:\n",
            "\n",
            "> update the `globals()` dictionary with the contents of the `vars(args)` dictionary\n",
            "= globals().update(vars(args))\n",
            "< globals((.update(vars(())))(())))\n",
            "\n",
            "> check if any of the items in  `search` appear in `string`\n",
            "= any(x in string for x in search)\n",
            "< any(x in string for x in search)\n",
            "\n",
            "> sort list `a` using the first dimension of the element as the key to list `b`\n",
            "= a.sort(key=lambda x: b.index(x[0]))\n",
            "< [a[ b] b b b]\n",
            "\n",
            "> count the number of items in a generator/iterator `it`\n",
            "= sum(1 for i in it)\n",
            "< sum(1 for i in it)\n",
            "\n",
            "> print the number of occurences of not `none` in a list `lst` in Python 2\n",
            "= print(len([x for x in lst if x is not None]))\n",
            "< print([(']))\n",
            "\n",
            "> create a file 'filename' with each tuple in the list `mylist` written to a line\n",
            "= open('filename', 'w').write('\\n'.join('%s %s' % x for x in mylist))\n",
            "< open('filename', 'w')':.'',,\n",
            "\n",
            "> extract dictionary values by key 'Feature3' from data frame `df`\n",
            "= feature3 = [d.get('Feature3') for d in df.dic]\n",
            "< feature3 =['(d:d(d(x,: x[])\n",
            "\n",
            "> Generate random integers between 0 and 9\n",
            "= print((random.randint(0, 9)))\n",
            "< print(( 9 9 9 9 9 9 9 9 9.randint, 9)\n",
            "\n",
            "> Get the zip output as list from the lists `[1, 2, 3]`, `[4, 5, 6]`, `[7, 8, 9]`\n",
            "= [list(a) for a in zip([1, 2, 3], [4, 5, 6], [7, 8, 9])]\n",
            "< [(*[[1,[,,,,,,,,,,,,,,,, ,,,,, ,,,,,,,,,,,,,,,,,)],,,))]))\n",
            "\n",
            "> get the ASCII value of a character 'a' as an int\n",
            "= ord('a')\n",
            "< ord('a')\n",
            "\n",
            "> decode encodeuricomponent in GAE\n",
            "= urllib.parse.unquote(h.path.encode('utf-8')).decode('utf-8')\n",
            "< urllib.parse.unquote(h.encode('utf-8'))\n",
            "\n",
            "> slice list `[1, 2, 3, 4, 5, 6, 7]` into lists of two elements each\n",
            "= list(grouper(2, [1, 2, 3, 4, 5, 6, 7]))\n",
            "< list([(1,, 4, 4, 4, 4(,,,, 4, 4,, 6),,,,, 6, 6) 6(, 2, 3)), ,,,,, ,,))\n",
            "\n",
            "> split a string 's' by space while ignoring spaces within square braces and quotes.\n",
            "= re.findall('\\\\[[^\\\\]]*\\\\]|\"[^\"]*\"|\\\\S+', s)\n",
            "< re.findall('\\\\[^\\\\]|\"\"\"\\\\|\"]*\"\"\"\n",
            "\n",
            "> specify multiple positional arguments with argparse\n",
            "= parser.add_argument('input', nargs='+')\n",
            "< parser.add_argument('input', nargs=',''')', text=''\"\n",
            "\n",
            "> round number 4.0005 up to 3 decimal places\n",
            "= round(4.0005, 3)\n",
            "< round(4.0005, 3)\n",
            "\n",
            "> read file 'myfile' using encoding 'iso-8859-1'\n",
            "= codecs.open('myfile', 'r', 'iso-8859-1').read()\n",
            "< codecs.open('myfile', 'U', 'rb')\n",
            "\n",
            "> ordering a list of dictionaries `mylist` by elements 'weight' and 'factor'\n",
            "= mylist.sort(key=lambda d: (d['weight'], d['factor']))\n",
            "< mylist.sort(key=lambda d: ['key'], d['factor'], reverse=True)\n",
            "\n",
            "> write pandas dataframe `df` to the file 'c:\\\\data\\\\t.csv' without row names\n",
            "= df.to_csv('c:\\\\data\\\\t.csv', index=False)\n",
            "< df.to_csv('c\\\\t\\\\t')\\\\:\n",
            "\n",
            "> print float `a` with two decimal points\n",
            "= print(('%.2f' % round(a, 2)))\n",
            "< print(('{02f.2f}'.format(round(a, 2)))))\n",
            "\n",
            "> write a tuple of tuples `A` to a csv file using python\n",
            "= writer.writerow(A)\n",
            "< writer.writerow(A)\n",
            "\n",
            "> count the number of words in a string `s`\n",
            "= len(s.split())\n",
            "< len(s.split())\n",
            "\n",
            "> get a new string from the 3rd character to the end of the string `x`\n",
            "= x[2:]\n",
            "< x[:(-2)]\n",
            "\n",
            "> get the indexes of the x and y axes in Numpy array `np` where variable `a` is equal to variable `value`\n",
            "= i, j = np.where(a == value)\n",
            "< i, j = np.where(a == value)\n",
            "\n",
            "> encode string \"\\\\xc3\\\\x85あ\" to bytes\n",
            "= \"\"\"\\\\xc3\\\\x85あ\"\"\".encode('utf-8')\n",
            "< \"\"\"\\\\xc3\\\"\")\n",
            "\n",
            "> Exit script\n",
            "= sys.exit()\n",
            "< sys.exit()\n",
            "\n",
            "> multiply the columns of sparse matrix `m` by array `a` then multiply the rows of the resulting matrix by array `a`\n",
            "= numpy.dot(numpy.dot(a, m), a)\n",
            "< numpy.dot(numpy.dot(a.T, a).dot(a.T, a)\n",
            "\n",
            "> Get Last Day of the first month in year 2000\n",
            "= (datetime.date(2000, 2, 1) - datetime.timedelta(days=1))\n",
            "< (datetimedatetimedatetime.utcnow, 2, datetime.timedelta-,,(.,,\n",
            "\n",
            "Correct on 352 snippets out of 2300 on training set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8105oJgS01_",
        "colab_type": "code",
        "outputId": "5d5135dc-c474-4ed6-befe-1609acd85080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "test_pairs = orginize_data(data_type=\"conala-test.json\")\n",
        "answers = evaluate_data_set(encoder1,attn_decoder1, test_pairs)\n",
        "snippet = [w[1] for w in test_pairs]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> send a signal `signal.SIGUSR1` to the current process\n",
            "= os.kill(os.getpid(), signal.SIGUSR1)\n",
            "< os.chdir('..)')\n",
            "\n",
            "> parse milliseconds epoch time '1236472051807' to format '%Y-%m-%d %H:%M:%S'\n",
            "= time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(1236472051807 / 1000.0))\n",
            "< datetime.datetime(combine=ms / 1000.date\n",
            "\n",
            "> create list `lst` containing 100 instances of object `Object`\n",
            "= lst = [Object() for i in range(100)]\n",
            "< [[[i][.[(:((]]]]]]]\n",
            "\n",
            "> convert index at level 0 into a column in dataframe `df`\n",
            "= df.reset_index(level=0, inplace=True)\n",
            "< df.groupby('.columns).agg(['] axis+', axis=0)\n",
            "\n",
            "> Removing duplicates in list `abracadabra`\n",
            "= list(OrderedDict.fromkeys('abracadabra'))\n",
            "< time.strftime(')\n",
            "\n",
            "> sort a list `l` of dicts by dict value 'title'\n",
            "= l.sort(key=lambda x: x['title'])\n",
            "< sorted(the_list, key=lambda x: x(x.split('.')[:])[(])))))\n",
            "\n",
            "> Django response with JSON `data`\n",
            "= return HttpResponse(data, mimetype='application/json')\n",
            "< data.rename('c')\n",
            "\n",
            "> make a barplot of data in column `group` of dataframe `df` colour-coded according to list `color`\n",
            "= df['group'].plot(kind='bar', color=['r', 'g', 'b', 'r', 'g', 'b', 'r'])\n",
            "< df[([]_[]('] ('])()])()])()]))]())]))])])\n",
            "\n",
            "> delete all characters \"i\" in string \"it is icy\"\n",
            "= \"\"\"it is icy\"\"\".replace('i', '')\n",
            "< re.findall('\\(|-[]+|Z])', s)\n",
            "\n",
            "> write dataframe `df`, excluding index, to a csv file\n",
            "= df.to_csv(filename, index=False)\n",
            "< df.set_name(name, ' ')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edq4DhrBy3M_",
        "colab_type": "code",
        "outputId": "1ec24155-fdd0-4d10-c4fe-4ee02fc66ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "c_snippet = [[c for c in s] for s in snippet]\n",
        "c_answers = [[c for c in s] for s in answers]\n",
        "\n",
        "score = 0\n",
        "acc = 0\n",
        "bleu_nltk = sentence_bleu\n",
        "cc = SmoothingFunction()\n",
        "# Compute score for every snippet and it's genarated code\n",
        "for i in range(len(c_answers)):\n",
        "  score += bleu_nltk([c_snippet[i]], c_answers[i], smoothing_function=cc.method4)\n",
        "  if c_snippet[i] == c_answers[i]:\n",
        "    acc+=1\n",
        "print(\"Correct on {} snippets out of {} on training set\".format(acc,len(c_answers)))\n",
        "print(\"Avg Bleu score is: {}\".format(score/len(answers)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct on 0 snippets out of 477 on training set\n",
            "Avg Bleu score is: 0.1357720257421837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uwgUipRfDOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "23371ba1-f203-4882-d3f8-dca90c0f108b"
      },
      "source": [
        "ls = [[\"check if item from list `b` is in list `a`\",\"print(x in a for x in b)\"]]\n",
        "n_lang = evaluate_data_set(encoder1,attn_decoder1, ls)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> check if item from list `b` is in list `a`\n",
            "= print(x in a for x in b)\n",
            "< print('['join()set(a))]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}